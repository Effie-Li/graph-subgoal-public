import glob
import torch
import pandas as pd

from data.dataset import GraphDataset, GraphDataModule
from model.model_module import GraphModelModule

def fetch_runs(api, project):
    '''
    uses wandb api to fetch information about all runs in a project
    args
    ----
        api : wandb api instance
        project : string
            the project repo

    returns
    -------
    runs_df : pandas dataframe
        containing runid, group, name, model config columns, and run summary columns
    '''

    runs = api.runs(project)

    summary_list, config_list, name_list, group_list, id_list = [], [], [], [], []
    for run in runs: 
        # .summary contains the output keys/values for metrics like accuracy.
        #  We call ._json_dict to omit large files 
        summary_list.append(run.summary._json_dict)

        # .config contains the hyperparameters.
        #  We remove special values that start with _.
        config_list.append(
            {k: v for k,v in run.config.items()
            if not k.startswith('_')})

        # .name is the human-readable name of the run.
        name_list.append(run.name)
        group_list.append(run.group)
        id_list.append(run.id)

    summary = pd.json_normalize(summary_list)
    config = pd.json_normalize(config_list)

    runs_df = pd.DataFrame({
        'id': id_list,
        'group': group_list,
        'name': name_list
    })

    runs_df = pd.concat([runs_df, config, summary], axis=1)
    return runs_df, config_list, id_list

def get_learning_curve_data(api, project, runid, keys):
    run = api.run(project + '/' + runid)
    run_history = run.scan_history(keys=keys)
    result = {k: [row[k] for row in run_history] for k in keys}
    result = pd.DataFrame.from_dict(result)
    return result

def get_ckptfs_from_run(ckpt_dir, runid):

    '''
    args
    ----
    ckpt_dir : string
        path to root directory containing all runs
    runid : string
        random run identifier generated by wandb

    returns
    -------
    ckpts : list of string
        filenames of all checkpoint files
    '''

    path = ckpt_dir + '/' + runid + '/checkpoints/'
    files = glob.glob(path+'*.ckpt')
    files = [f for f in files if ('last' not in f and 'init' not in f) ]
    strs = [file.split('/')[-1][:-5] for file in files] # extract the 'epoch=X-step=XXX' part in the fname
    epochs = [int(s.split('-')[0].split('=')[1]) for s in strs]
    steps = [int(s.split('-')[1].split('=')[1]) for s in strs]
    ckpts = pd.DataFrame({
        'epoch': epochs,
        'step': steps,
        'fname': files
    })
    ckpts.sort_values(['epoch','step'], inplace=True)
    ckpts.reset_index(drop=True, inplace=True)
    return ckpts

def recreate_datamodule(config, batch_size):
    if 'n_node' not in config['dataset']: # for backwards compatibility
        if 'human30' in config['dataset']['fname']:
            config['dataset']['n_node'] = 8
        elif 'random50-node=15' in config['dataset']['fname']:
            config['dataset']['n_node'] = 15
    ds = GraphDataset(fname=config['dataset']['fname'], 
                      n_graph=config['dataset']['n_graph'], 
                      n_isomorph=config['dataset']['n_isomorph'],
                      n_node=config['dataset']['n_node'],
                      include_graphs=config['dataset']['include_graphs'] if 'include_graphs' in config['dataset'] else None)
    dm = GraphDataModule(dataset=ds, 
                         batch_size=batch_size, 
                         split_spec=config['dataset']['split_params'], 
                         train_idx=config['dataset']['train_idx'], 
                         val_idx=config['dataset']['val_idx'])
    return dm

def load_model_and_val_batch(config, ckpt_dir, ckpt='last', val_batch='all'):
    '''
    ckpt: 'last' or 'best'
    val_batch: 'all' or 'len4+'
    '''

    # model checkpoints
    if ckpt == 'last':
        ckpt = f'{ckpt_dir}/last.ckpt'
    elif ckpt == 'best':
        ckpt = glob.glob(f'{ckpt_dir}/epoch=*.ckpt')[0]
    model = GraphModelModule.load_from_checkpoint(ckpt)
    _ = model.eval()

    # dataset (with intentionally large batch size for detailed evaluation)
    bsz = 10000 if config['dataset']['name'] == 'human30' else 40000
    dm = recreate_datamodule(config, batch_size=bsz)
    for batch in dm.val_dataloader(): break

    # skip filler trials
    val_indices = batch['index'][batch['path_len']!=2].numpy().tolist()
    dm.val_idx = val_indices
    dm.data_val = torch.utils.data.Subset(dm.dataset, val_indices)
    for batch in dm.val_dataloader():break

    # shrink val batch size here for desired evaluation on subset of problems
    if val_batch == 'len4+':
        val_indices = batch['index'][batch['path_len']>=4].numpy().tolist()
        dm.val_idx = val_indices
        dm.data_val = torch.utils.data.Subset(dm.dataset, val_indices)
        for batch in dm.val_dataloader():break
    
    return model, dm, batch